df=read.csv('EXO1.csv')
ds<-lm(Y ~ X_1 ,data=df)
summary(ds)
## 
## Call:
## lm(formula = Y ~ X_1, data = df)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -8.266 -4.887 -1.208  3.232 10.770 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)    
## (Intercept)    3.523      4.383   0.804 0.440237    
## X_1            6.036      1.279   4.721 0.000816 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 6.633 on 10 degrees of freedom
## Multiple R-squared:  0.6903, Adjusted R-squared:  0.6593 
## F-statistic: 22.29 on 1 and 10 DF,  p-value: 0.0008155
dk<-lm(Y ~ X_2,data=df )
summary(dk)
## 
## Call:
## lm(formula = Y ~ X_2, data = df)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -15.1923  -5.1780  -0.2298   6.1123  12.3077 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)  
## (Intercept)  -36.373     20.489  -1.775   0.1062  
## X_2           17.464      6.069   2.878   0.0164 *
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 8.815 on 10 degrees of freedom
## Multiple R-squared:  0.453,  Adjusted R-squared:  0.3983 
## F-statistic: 8.282 on 1 and 10 DF,  p-value: 0.01645
dg<-lm(Y ~ X_1+X_2,data=df )
summary(dg)
## 
## Call:
## lm(formula = Y ~ X_1 + X_2, data = df)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -6.897 -2.135 -1.126  1.714 10.122 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)    
## (Intercept)  -30.081     11.455  -2.626 0.027542 *  
## X_1            4.905      1.014   4.838 0.000923 ***
## X_2           11.072      3.621   3.058 0.013617 *  
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 4.897 on 9 degrees of freedom
## Multiple R-squared:  0.8481, Adjusted R-squared:  0.8143 
## F-statistic: 25.12 on 2 and 9 DF,  p-value: 0.0002075
pourcentage de variation
Pour ds: 69.03%

R2Y ;X1 = SCreg SCtot = 980; 64 1420; 67 = 0; 6903:

Pour dk: 45.3%

Pour dg: 84.81%

Tester
shapiro.test(residuals(dg))
## 
##  Shapiro-Wilk normality test
## 
## data:  residuals(dg)
## W = 0.94078, p-value = 0.5082
Calculons la statistique du test de Fisher observée qui est égale à :
Fobs =SCreg=ddl/SCres=ddl=(1204,86/2)/(215,81/(12-2-1 = 9)=25,123:

Le quantile de loi de Fisher critique lu dans la table des quantiles de la loi de Fisher à 95% est égal à :

Fc;2;9 = 4,256495

La statistique du test de Fisher observée est plus grande que le quantile de loi de Fisher critique. Donc nous sommes dans la zone de rejet de l’hypothèse nulle H0. Donc nous décidons de refuser l’hypothèse nulle H0 et par conséquent d’accepter l’hypothèse alternative H1, c’est-à-dire :

quelque soit j = 1; ou 2; Bj n’egale pas 0

intérvalle de confiance
confint(ds)
##                 2.5 %   97.5 %
## (Intercept) -6.242858 13.28806
## X_1          3.187036  8.88479
confint(dk)
##                  2.5 %   97.5 %
## (Intercept) -82.024671  9.27949
## X_2           3.942487 30.98642
confint(dg)
##                  2.5 %    97.5 %
## (Intercept) -55.994798 -4.167046
## X_1           2.611442  7.197983
## X_2           2.881287 19.262828
Q5
La régression est significative entre la résistance à la rupture et l’épaisseur du matériau si le test de Student qui teste si 1 = 0 n’est pas vérifié. Calculons la statistique du test de Student observée : tobs = 6,036/1,279= 4,721

Le quantile de la loi de Student critique lu dans une table des quantiles de la loi de Student à 95% est égal à : tc;95% = 2,228

La statistique du test de Student observée est plus grande que le quantile de la loi de Student critique. Par conséquent nous sommes dans la zone de rejet de l’hypothèse nulle H0. Donc nous décidons de refuser l’hypothèse nulle H0 et par conséquent d’accepter l’hypothèse alternative H1. Donc la régression est significative entre la résistance à la rupture et l’épaisseur du matériau.

summary(ds)
## 
## Call:
## lm(formula = Y ~ X_1, data = df)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -8.266 -4.887 -1.208  3.232 10.770 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)    
## (Intercept)    3.523      4.383   0.804 0.440237    
## X_1            6.036      1.279   4.721 0.000816 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 6.633 on 10 degrees of freedom
## Multiple R-squared:  0.6903, Adjusted R-squared:  0.6593 
## F-statistic: 22.29 on 1 and 10 DF,  p-value: 0.0008155
Cette pvaleur est égale à 0.000816, qui est inférieur à 5%. Donc même conclusion qu’en faisant les calculs à la main précédents.

Q6
predict(dg,data.frame(X_1 = 4,X_2 = 3.8),se.fit=TRUE)
## $fit
##        1 
## 31.61175 
## 
## $se.fit
## [1] 2.100369
## 
## $df
## [1] 9
## 
## $residual.scale
## [1] 4.896814
predict(dg,data.frame(X_1 = 4,X_2 = 3.4),se.fit=TRUE)
## $fit
##        1 
## 27.18292 
## 
## $se.fit
## [1] 1.664866
## 
## $df
## [1] 9
## 
## $residual.scale
## [1] 4.896814
predict(dg,data.frame(X_1 = 4,X_2 = 2.9),se.fit=TRUE)
## $fit
##        1 
## 21.64689 
## 
## $se.fit
## [1] 2.573244
## 
## $df
## [1] 9
## 
## $residual.scale
## [1] 4.896814
Q7
predict(dg,data.frame(X_1 = 4,X_2 = 3.8),interval="confidence")
##        fit      lwr      upr
## 1 31.61175 26.86038 36.36311
EX02
ex02=read.csv('EXO2.csv')
Question 1
s<-lm(Y ~ X_1 ,data=ex02)
summary(s)
## 
## Call:
## lm(formula = Y ~ X_1, data = ex02)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -24.157 -14.190  -2.637  12.219  32.762 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)    
## (Intercept) 46.23633    5.77111   8.012 1.14e-07 ***
## X_1          0.12002    0.05044   2.379   0.0274 *  
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 17.47 on 20 degrees of freedom
## Multiple R-squared:  0.2206, Adjusted R-squared:  0.1817 
## F-statistic: 5.662 on 1 and 20 DF,  p-value: 0.02741
shapiro.test(residuals(s))
## 
##  Shapiro-Wilk normality test
## 
## data:  residuals(s)
## W = 0.95492, p-value = 0.3938
Question 2
g<-lm(Y ~ X_1+X_2,data=ex02 )
summary(g)
## 
## Call:
## lm(formula = Y ~ X_1 + X_2, data = ex02)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -22.311 -13.582  -3.174  14.342  32.402 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)  
## (Intercept) 35.55464   16.56005   2.147   0.0449 *
## X_1          0.07895    0.07849   1.006   0.3271  
## X_2          0.47073    0.68276   0.689   0.4989  
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 17.7 on 19 degrees of freedom
## Multiple R-squared:  0.2397, Adjusted R-squared:  0.1596 
## F-statistic: 2.994 on 2 and 19 DF,  p-value: 0.07407
Alos l’équation de la régression linéaire multiple de Y sur X1 et X2 est :

Y= 35.055464 +0.07895X_1 + 0.47073X_2

Question 3:
L’hypothèse nulle H0 : = 0 contre l’hypothèse alternative H1 : = 0: Calculons la statistique du test de Fisher observée qui est égale à : Fobs =SCreg=ddl/SCres=ddl=(190,232/1)/571,723=(22-2-1 = 19)= 6,332 Le quantile de la loi de Fisher critique lu dans la table des quantiles de la loi de Fisher à 95% est égal à : Fc;1;19 = 4; 38075:

Question 4
k<-lm(Y ~ X_1+X_2+X_3,data=ex02 )
summary(k)
## 
## Call:
## lm(formula = Y ~ X_1 + X_2 + X_3, data = ex02)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -24.134 -10.675  -1.435   9.321  29.800 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(>|t|)  
## (Intercept) 23.999566  15.978866   1.502   0.1504  
## X_1         -0.006173   0.081298  -0.076   0.9403  
## X_2         -0.479869   0.757034  -0.634   0.5341  
## X_3          8.483500   3.846205   2.206   0.0406 *
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 16.13 on 18 degrees of freedom
## Multiple R-squared:  0.4014, Adjusted R-squared:  0.3017 
## F-statistic: 4.024 on 3 and 18 DF,  p-value: 0.02357
question 5
confint(g)
##                   2.5 %     97.5 %
## (Intercept)  0.89406467 70.2152187
## X_1         -0.08534121  0.2432419
## X_2         -0.95829665  1.8997550
question 6
predict(dg,data.frame(X_1 = 221,X_2 = 39, X_3=7),interval="confidence")
##        fit      lwr      upr
## 1 1485.671 1007.634 1963.707
quetion 8
h<-lm(X_1 ~ X_2+X_3,data=ex02 )
summary(h)
## 
## Call:
## lm(formula = X_1 ~ X_2 + X_3, data = ex02)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -64.734 -31.628  -0.529  29.773  90.961 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)   
## (Intercept) -117.910     36.076  -3.268  0.00404 **
## X_2            2.597      2.052   1.266  0.22087   
## X_3           22.459      9.553   2.351  0.02967 * 
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 45.53 on 19 degrees of freedom
## Multiple R-squared:  0.6715, Adjusted R-squared:  0.6369 
## F-statistic: 19.42 on 2 and 19 DF,  p-value: 2.553e-05
q9
b<-lm(X_1 ~ X_3,data=ex02 )
summary(b)
## 
## Call:
## lm(formula = X_1 ~ X_3, data = ex02)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -77.724 -33.602   1.776  24.397  89.779 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)    
## (Intercept)  -95.759     32.019  -2.991  0.00723 ** 
## X_3           32.498      5.405   6.012 7.05e-06 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 46.21 on 20 degrees of freedom
## Multiple R-squared:  0.6438, Adjusted R-squared:  0.626 
## F-statistic: 36.15 on 1 and 20 DF,  p-value: 7.051e-06
confint(b)
##                  2.5 %    97.5 %
## (Intercept) -162.54946 -28.96795
## X_3           21.22245  43.77258